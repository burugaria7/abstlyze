{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-02T16:30:46.106674900Z",
     "start_time": "2023-10-02T16:30:41.039175600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Re\\Documents\\GitHub\\abstlyze\\venv\\lib\\site-packages\\umap\\distances.py:1063: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Re\\Documents\\GitHub\\abstlyze\\venv\\lib\\site-packages\\umap\\distances.py:1071: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Re\\Documents\\GitHub\\abstlyze\\venv\\lib\\site-packages\\umap\\distances.py:1086: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  @numba.jit()\n",
      "C:\\Users\\Re\\Documents\\GitHub\\abstlyze\\venv\\lib\\site-packages\\umap\\umap_.py:660: NumbaDeprecationWarning: \u001B[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001B[0m\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a concept of multi valued cognitive maps is introduced in this paper the concept expands the fuzzy one however all variables and weights are not linearly ordered in the concept but are only partially ordered such an approach allows us to operate in cognitive maps with partially ordered linguistic variables directly without vague fuzzification defuzzification methods hence we may consider more subtle differences in degrees of experts uncertainty than in the fuzzy case we prove the convergence of such cognitive maps and give two simple computational examples which demonstrate using such a partially ordered uncertainty degree scale compared to the fuzzy case'\n",
      " 'in this paper we investigate the fixed time synchronization for fuzzy inertial neural networks with time varying coefficients and time delays the fuzzy inertial neural networks are transformed into two forms of first order differential systems and then two kinds of different controllers of time variable are designed in these schemes a series of novel criteria are proposed to ensure the fixed time synchronization for the drive response fuzzy inertial neural networks via state feedback control methods sufficient conditions of delay dependent and delay independent are obtained in terms of algebraic inequalities not the matrix inequalities moreover these new criteria allow the derivative of the lyapunov function can be positive but not negative definite as the earlier work and the algebraic conditions become easy to be tested under the new criteria finally two examples are added to illustrate effectiveness of the results we obtained meanwhile a new image encryption algorithm is proposed based on the synchronization of drive response networks and the effectiveness of this algorithm is also demonstrated'\n",
      " 'for most bayesian inference problems that are of interest solving for the model parameter posterior probability distribution remains to be the main challenge a typical technique entails using markov chain monte carlo mcmc in order to obtain an approximation of the posterior distribution however one drawback of mcmc is that it usually requires solving the forward problem multiple times which could be infeasible if the forward model is computationally expensive in this paper a computationally cheap surrogate model is developed using the adaptive neuro fuzzy inference system with a fuzzy c means initialization anfis fcm the optimal number of fuzzy rules is chosen via a proposed algorithm based on the pairing frequency clustering validity index the effectiveness of the anfis fcm model is tested on inferring reaction rate parameters in a node network toy problem which mimics idealized combustion reaction networks using multiple error measures it is shown that the developed anfis fcm model is capable of constructing accurate surrogate models at a minimal computational cost'\n",
      " ...\n",
      " 'in the recent past numerous models and methods have been suggested to solve the vectormaximum problem most of these approaches center their attention on linear programming problems with several objective functions apart from these approaches the theory of fuzzy sets has been employed to formulate and solve fuzzy linear programming problems this paper presents the application of fuzzy linear programming approaches to the linear vectormaximum problem it shows that solutions obtained by fuzzy linear programming are always efficient solutions it also shows the consequences of using different ways of combining individual objective functions in order to determine an optimal compromise solution'\n",
      " 'the complexity of experience acquired through our senses and as interpreted by our mind is fuzzy and must remain so as long as the meaning of things change as they are embedded in larger or different contexts to relate them to new ideas and new experiences here we give a method for measuring the relativity of fuzziness by structuring the functions of a system hierarchically in a multiple objective framework the method is based on computing the principal eigenvector of a positive matrix with reciprocal entries i e aji aij the eigenvector provides an estimate for an assumed underlying ratio scale for a single property the scale provides a measure of the grade of membership of elements in a fuzzy set according to that property and the departure of the eigenvalue from the dimension of the matrix serves as a measure of departure from consistency for a number of properties the principle of hierarchical composition enables us to compose the eigenvectors into a priority vector which measures the fuzziness of the elements in the lowest level of the hierarchy with respect to the relative dominance of the purposes or properties represented in the hierarchy'\n",
      " 'when r is a fuzzy relation between the elements of a finite set x the fuzzy subsets a of x such that r a a max min composition are called eigen fuzzy sets in this paper we describe algorithms for the determination of the greatest eigen fuzzy set associated with a given fuzzy relation thinking of medical diagnosis applications']\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "data_set_path = pathlib.Path(\"dataset/fsas/fsas_full_utf8_dated.csv\")\n",
    "trump = pd.read_csv(str(data_set_path))\n",
    "trump.abstract = trump.apply(lambda row: re.sub(r\"http\\S+\", \"\", row.abstract).lower(), 1)\n",
    "trump.abstract = trump.apply(lambda row: \" \".join(filter(lambda x: x[0] != \"@\", row.abstract.split())), 1)\n",
    "trump.abstract = trump.apply(lambda row: \" \".join(re.sub(\"[^a-zA-Z]+\", \" \", row.abstract).split()), 1)\n",
    "# trump = trump.loc[(trump.isRetweet == \"f\") & (trump.abstract != \"\"), :]\n",
    "timestamps = trump.date.to_list()\n",
    "abstracts = trump.abstract.to_list()\n",
    "\n",
    "data = abstracts\n",
    "\n",
    "arr_multi = np.array(abstracts)\n",
    "print(arr_multi)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T16:32:21.194372900Z",
     "start_time": "2023-10-02T16:32:20.616373400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 39\u001B[0m\n\u001B[0;32m     27\u001B[0m topic_model \u001B[38;5;241m=\u001B[39m BERTopic(\n\u001B[0;32m     28\u001B[0m     embedding_model\u001B[38;5;241m=\u001B[39membedding_model,  \u001B[38;5;66;03m# Step 1 - Extract embeddings\u001B[39;00m\n\u001B[0;32m     29\u001B[0m     umap_model\u001B[38;5;241m=\u001B[39mumap_model,  \u001B[38;5;66;03m# Step 2 - Reduce dimensionality\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     34\u001B[0m     min_topic_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m7\u001B[39m  \u001B[38;5;66;03m# Create more topics\u001B[39;00m\n\u001B[0;32m     35\u001B[0m )\n\u001B[0;32m     37\u001B[0m topics, probs \u001B[38;5;241m=\u001B[39m topic_model\u001B[38;5;241m.\u001B[39mfit_transform(data)\n\u001B[1;32m---> 39\u001B[0m new_topics \u001B[38;5;241m=\u001B[39m \u001B[43mtopic_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce_outliers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtopics\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprobabilities\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[43m                                         \u001B[49m\u001B[43mthreshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.05\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprobabilities\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     41\u001B[0m topic_model\u001B[38;5;241m.\u001B[39mupdate_topics(data, topics\u001B[38;5;241m=\u001B[39mnew_topics)\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\abstlyze\\venv\\lib\\site-packages\\bertopic\\_bertopic.py:2102\u001B[0m, in \u001B[0;36mBERTopic.reduce_outliers\u001B[1;34m(self, documents, topics, images, strategy, probabilities, threshold, embeddings, distributions_params)\u001B[0m\n\u001B[0;32m   2100\u001B[0m \u001B[38;5;66;03m# Reduce outliers by extracting most likely topics through the topic-term probability matrix\u001B[39;00m\n\u001B[0;32m   2101\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m strategy\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprobabilities\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m-> 2102\u001B[0m     new_topics \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39margmax(prob) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mmax\u001B[39m(prob) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m threshold \u001B[38;5;129;01mand\u001B[39;00m topic \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m topic\n\u001B[0;32m   2103\u001B[0m                   \u001B[38;5;28;01mfor\u001B[39;00m topic, prob \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(topics, probabilities)]\n\u001B[0;32m   2105\u001B[0m \u001B[38;5;66;03m# Reduce outliers by extracting most frequent topics through calculating of Topic Distributions\u001B[39;00m\n\u001B[0;32m   2106\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m strategy\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdistributions\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\abstlyze\\venv\\lib\\site-packages\\bertopic\\_bertopic.py:2102\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   2100\u001B[0m \u001B[38;5;66;03m# Reduce outliers by extracting most likely topics through the topic-term probability matrix\u001B[39;00m\n\u001B[0;32m   2101\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m strategy\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprobabilities\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m-> 2102\u001B[0m     new_topics \u001B[38;5;241m=\u001B[39m [np\u001B[38;5;241m.\u001B[39margmax(prob) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprob\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m threshold \u001B[38;5;129;01mand\u001B[39;00m topic \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m topic\n\u001B[0;32m   2103\u001B[0m                   \u001B[38;5;28;01mfor\u001B[39;00m topic, prob \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(topics, probabilities)]\n\u001B[0;32m   2105\u001B[0m \u001B[38;5;66;03m# Reduce outliers by extracting most frequent topics through calculating of Topic Distributions\u001B[39;00m\n\u001B[0;32m   2106\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m strategy\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdistributions\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mTypeError\u001B[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine',\n",
    "                  # random_state=42,\n",
    "                  )\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True,\n",
    "                        min_samples=5)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representations with\n",
    "# a `bertopic.representation` model\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,  # Step 1 - Extract embeddings\n",
    "    umap_model=umap_model,  # Step 2 - Reduce dimensionality\n",
    "    hdbscan_model=hdbscan_model,  # Step 3 - Cluster reduced embeddings\n",
    "    vectorizer_model=vectorizer_model,  # Step 4 - Tokenize topics\n",
    "    ctfidf_model=ctfidf_model,  # Step 5 - Extract topic words\n",
    "    representation_model=representation_model,  # Step 6 - (Optional) Fine-tune topic represenations\n",
    "    min_topic_size=7  # Create more topics\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(data)\n",
    "\n",
    "new_topics = topic_model.reduce_outliers(data, topics, probabilities=probs,\n",
    "                                         threshold=0.05, strategy=\"probabilities\")\n",
    "topic_model.update_topics(data, topics=new_topics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-02T16:35:01.719377600Z",
     "start_time": "2023-10-02T16:32:26.604877400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
